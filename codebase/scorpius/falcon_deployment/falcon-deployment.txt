Automated deployment for DeepCortex platform

If you plan to import existing IAM resources, you must have all IAM resources created ahead of time. Check the iam_resources.txt file to see what needs to be created.

If you will be running this in an environment without an internet connection you must place all necessary artifacts on S3 ahead of time and list this bucket in the main config file as the artifacts_s3_bucket.

The ami to be used for deployment should be RHEL based and have the following packages installed:

curl tar xz zip unzip ipset ntp cloud-init kernel-devel-$(uname -r) gettext

Running the entire deployment

1) Create a folder called environments on your computer.

2) Place the deepcortex-settings.tfvars file under this "environmnets" directory.

3) Fill in the necessary variables in deepcortex-settings.tfvars.

4) If you would like to use a custom endpoints.json file for the AWS CLI have that file ready. (This file will overwrite any existing endpoints.json file on your AMIs, so leave this blank if you don't want to overwrite the existing file.)

5) Export the following variables in your terminal. The this can be done by filling in envar.sh with the proper values and then sourcing this file from your terminal (source envar.sh).

* CONFIG - (required) the name of the config file to use (should be deepcortex-settings).
* DCOS_USERNAME - (required) the username you'd like to use to login to the DC/OS cluster
* DCOS_PASSWORD - (required) the password you'd like to use to login to the DC/OS cluster (avoid special characters used in bash such as "#", ";", "$", etc.)
* AWS_ACCESS_KEY_ID (optional, only needed if not using IAM roles for terraform and packer) - the access key that should be used to deploy in AWS.
* AWS_SECRET_ACCESS_KEY (optional, only needed if not using IAM roles for terraform and packer) - the secret key that should be used to deploy in AWS.
* DOCKER_REGISTRY_AUTH_TOKEN - (optional, only needed for publci deployments) the token for docker authentication
* DOCKER_EMAIL_LOGIN - (optional, only needed for publci deployments) the login email for docker
* APPS_AWS_ACCESS_KEY_ID - (optional, only needed if using IAM user over roles) the access key for the app user.
* APPS_AWS_SECRET_ACCESS_KEY - (optional, only needed if using IAM user over roles) the secret key for the app user.

6) Run "docker login -u falcondeepcortex" and enter the password: 6apXmvzquPKGDwzx.

7) Run "docker pull deepcortex/scorpius-deployment:falcon".

=== Additional steps for getting the installer in C2S ===
7.a) In an environment with access to the internet run #7 above
7.b) Export the image "docker save --output <localpath>/falcon.tar deepcortex/scorpius-deployment:<docker_version>"
7.c) Copy the falcon.tar image to S3 along with all the other falcon assets.
7.d) Once files are transferred
7.e) Copy the tar file from S3 to the deployment machine.
7.f) Load it into docker "docker load -i <localpath>/falcon.tar"
========================================================

8) Run the following docker command replacing /path/to/environments with the path to the environments directory you created in step 1.

* Basic setup without passing any optional arguments:

docker run \
  -v </path/to/environments>:/opt/deploy/environments \
  -v /path/to/ssh-dir:/root/.ssh \
  -e CONFIG=${CONFIG} \
  -e DCOS_USERNAME=${DCOS_USERNAME} \
  -e DCOS_PASSWORD=${DCOS_PASSWORD} \
  deepcortex/scorpius-deployment:<docker_version>

* All options shown here:

docker run \
  -v </path/to/environments>:/opt/deploy/environments \
  -v /path/to/ssh-dir:/root/.ssh \
  -v </path/to/endpoints.json>:/opt/deploy/ansible/common/files/extra_files \
  -e CONFIG=${CONFIG} \
  -e AWS_ACCESS_KEY_ID=${AWS_ACCESS_KEY_ID} \
  -e AWS_SECRET_ACCESS_KEY=${AWS_SECRET_ACCESS_KEY} \
  -e CUSTOMER_KEY=${CUSTOMER_KEY} \
  -e DCOS_USERNAME=${DCOS_USERNAME} \
  -e DCOS_PASSWORD=${DCOS_PASSWORD} \
  -e DOCKER_EMAIL_LOGIN=${DOCKER_EMAIL_LOGIN} \
  -e DOCKER_REGISTRY_AUTH_TOKEN=${DOCKER_REGISTRY_AUTH_TOKEN} \
  -e DOCKER_EMAIL_LOGIN=${DOCKER_EMAIL_LOGIN} \
  -e DOCKER_REGISTRY_AUTH_TOKEN=${DOCKER_REGISTRY_AUTH_TOKEN} \
  deepcortex/scorpius-deployment:<docker_version>

9) Once your terminal output states the deployment is complete you can access the DeepCortex UI.

### Additional Docker Commands ###

Destroying the entire deployment
Everything, except the S3 buckets and any imported resources (e.g. VPC, IAM) will be destroyed.
By default, the S3 buckets are not destroyed, but can be manualy destroyed by passing the -t argument (see below).

docker run \
  -v ~/Desktop/environments:/opt/deploy/environments \
  -v /path/to/ssh-dir:/root/.ssh \
  -e CONFIG=${CONFIG} \
  -e CUSTOMER_KEY=${CUSTOMER_KEY} \
  -e DCOS_USERNAME=${DCOS_USERNAME} \
  -e DCOS_PASSWORD=${DCOS_PASSWORD} \
  -it --entrypoint "/bin/bash" deepcortex/scorpius-deployment:<docker_version> /opt/deploy/destroy.sh

Building specific stacks (e.g to build just the DC/OS cluster supply "platform" as STACKS)

docker run \
  -v ~/Desktop/environments:/opt/deploy/environments \
  -v /path/to/ssh-dir:/root/.ssh  \
  -v </path/to/endpoints.json>:/opt/deploy/ansible/common/files/extra_files \
  -e CONFIG=${CONFIG} \
  -e CUSTOMER_KEY=${CUSTOMER_KEY} \
  -e DCOS_USERNAME=${DCOS_USERNAME} \
  -e DCOS_PASSWORD=${DCOS_PASSWORD} \
  -it --entrypoint "/bin/bash" deepcortex/scorpius-deployment:<docker_version> /opt/deploy/build.sh -s STACKS

all optional arguments:
  -b: shutdown boostrap - can be set to true destroy bootstrap node after the cluster deploys
  -g: gpu on start - can be set to false to exclude spinning up a gpu node after the cluster deploys
  -m: deploy mode - can be set to simple to exclude download of DC/OS cli and extra output
  -s: stacks - a list of comma separated values to overwrite which terraform stacks to build
  -p: packer - can be set to false to exclude packer builds (helpful to avoid rebuilding them if they are already built)
  -t: s3 type - can be set to existing to import existing S3 buckets

Destroying specific stacks (e.g to destroy just the DC/OS cluster supply "platform" as STACKS)

docker run \
  -v ~/Desktop/environments:/opt/deploy/environments \
  -v /path/to/ssh-dir:/root/.ssh \
  -e CONFIG=${CONFIG} \
  -e CUSTOMER_KEY=${CUSTOMER_KEY} \
  -e DCOS_USERNAME=${DCOS_USERNAME} \
  -e DCOS_PASSWORD=${DCOS_PASSWORD} \
  -it --entrypoint "/bin/bash" deepcortex/scorpius-deployment:<docker_version> /opt/deploy/destroy.sh -s STACKS

all optional arguments:
  -s: stacks - a list of comma separated values to overwrite which terraform stacks to destroy
  -d: can be set to true to delete s3 buckets

### DC Lite ###

1) Download the model you'd like to use from the DeepCortex UI by clicking the "Download" button on the page for that model.
2) Pull the docker image deepcortex/cortex-tasks-dclite:<docker_version>.
3) Run one of the following commands:

  Local data:

  docker run -ti -v /path/to/models:/models -v /path/to/data:/data deepcortex/cortex-tasks-dclite:<docker_version> --model-file /models/model_file --image-format sar --input-path /data

  S3 data:

  docker run -ti -v /path/to/models:/models deepcortex/cortex-tasks-dclite:<docker_version> --model-file /models/model_file --image-format sar --input-path /S3/path --access-key AWS_ACCESS_KEY_ID --secret-key AWS_SECRET_ACCESS_KEY --region AWS_REGION --input-bucket AWS_S3_BUCKET



