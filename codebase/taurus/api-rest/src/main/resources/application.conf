taurus-service {
  name = "taurus-api"
  version = "v1"
}

stream-id = "8e25ced2-8fe4-4eae-8754-189a9af94f4a"
stream-id = ${?STREAM_ID}

# Options: OFF, ERROR, WARNING, INFO, DEBUG
akka-loglevel = "DEBUG"
akka-loglevel = ${?AKKA_LOGLEVEL}

akka {
  loglevel = ${akka-loglevel}

  log-dead-letters = 10
  log-dead-letters-during-shutdown = on
  jvm-exit-on-fatal-error = off

  loggers = ["akka.event.slf4j.Slf4jLogger"]
  logging-filter = "akka.event.slf4j.Slf4jLoggingFilter"

  actor {
    debug {
      receive = on
      autoreceive = on
      lifecycle = on
      event-stream = off
    }
    info {
      receive = off
      autoreceive = off
      lifecycle = off
      event-stream = off
    }
  }

  http {
    server {
      server-header = ${taurus-service.name}/${taurus-service.version}
      idle-timeout = 60 s
      request-timeout = 120 s
      max-connections = 1024
    }

    client {
      connecting-timeout = 120 s
      // Note: setting host-connection-pool.idle-timeout to infinite until
      // https://github.com/akka/akka-http/issues/1245 gets merged in a release version.
      host-connection-pool.idle-timeout = infinite
    }

    host-connection-pool {
      max-connections = 1024
      max-retries = 3
      max-open-requests = 16384
    }
  }

  persistence.journal {
    plugin = "akka-contrib-mongodb-persistence-journal"
  }

  contrib.persistence.mongodb {
    mongo {
      mongouri = "mongodb://localhost"
      mongouri = ${?AKKA_PERSISTENCE_MONGODB_CREDENTIAL_DATABASE_URL}

      database = "taurus"
      database = ${?AKKA_PERSISTENCE_MONGODB_JOURNAL_DATABASE_NAME}

      journal-collection = "akka-persistence-journal"
      journal-collection = ${?AKKA_PERSISTENCE_MONGODB_JOURNAL_COLLECTIONS_NAME_PREFIX}

      journal-index = "akka-persistence-journal-index"
      journal-index = ${?AKKA_PERSISTENCE_MONGODB_JOURNAL_INDEX_NAME}

      journal-write-concern = "Acknowledged"

      suffix-builder {
        separator = "_"
        class = "taurus.util.akka.persistence.mongo.PersistenceCollectionSuffixBuilder"
      }
    }

    rxmongo.failover {
      initialDelay = 50ms
      retries = 10
      growth = exp
      factor = 1.5
    }
  }
}

http {
  interface = "0.0.0.0"
  port = 9000
}

hdfs-client {
  host = "0.0.0.0"
  port = 8020
}

s3-client {
  access-key = "access-key"
  access-key = ${?S3_ACCESS_KEY}

  secret-key = "secret-key"
  secret-key = ${?S3_SECRET_KEY}

  region = "us-east-1"
  region = ${?S3_REGION}

  bucket-name = "dev.deepcortex.ai"
  bucket-name = ${?S3_BUCKET}
}

sqs-client {
  access-key = ${s3-client.access-key}
  access-key = ${?SQS_ACCESS_KEY}

  secret-key = ${s3-client.secret-key}
  secret-key = ${?SQS_SECRET_KEY}

  region = ${?s3-client.region}
  region = ${?SQS_REGION}

  queue-name = "online-prediction-default"
  queue-name = ${?SQS_QUEUE_NAME}
}

op-rabbit {
  topic-exchange-name = "ml_job.gateway"
  channel-dispatcher = "op-rabbit.default-channel-dispatcher"
  default-channel-dispatcher {
    # Dispatcher is the name of the event-based dispatcher
    type = Dispatcher

    # What kind of ExecutionService to use
    executor = "fork-join-executor"

    # Configuration for the fork join pool
    fork-join-executor {
      # Min number of threads to cap factor-based parallelism number to
      parallelism-min = 2

      # Parallelism (threads) ... ceil(available processors * factor)
      parallelism-factor = 2.0

      # Max number of threads to cap factor-based parallelism number to
      parallelism-max = 4
    }
    # Throughput defines the maximum number of messages to be
    # processed per actor before the thread jumps to the next actor.
    # Set to 1 for as fair as possible.
    throughput = 10
  }
  connection {
    virtual-host = "/"

    hosts = ["0.0.0.0"]
    hosts = [${?RABBIT_HOSTS}]

    username = "guest"
    username = ${?RABBIT_USERNAME}

    password = "guest"
    password = ${?RABBIT_PASSWORD}

    port = 5672
    ssl = false
    connection-timeout = 3s
  }
}

cortex {
  rest-url = "http://0.0.0.0:9000"
  rest-url = ${?CORTEX_REST_URL}

  api-version = "v1"
  api-version = ${?CORTEX_REST_VERSION}

  credentials {
    username = "username"
    username = ${?CORTEX_USERNAME}

    password = "password"
    password = ${?CORTEX_PASSWORD}
  }

  input-directory = "/tmp/deepcortex/jobs"
  request-retry-count = 3

  job-polling {
    max-attempts = 1200
    interval = 30 seconds
  }
}

aries {
  rest-url = "http://0.0.0.0:9000"
  rest-url = ${?ARIES_REST_URL}

  api-version = "v1"
  api-version = ${?ARIES_REST_VERSION}

  credentials {
    username = "username"
    username = ${?ARIES_USERNAME}

    password = "password"
    password = ${?ARIES_PASSWORD}
  }

  request-retry-count = 3
}

baile {
  rest-url = "http://0.0.0.0:9000"
  rest-url = ${?BAILE_REST_URL}

  api-version = "v1.4"
  api-version = ${?BAILE_REST_VERSION}

  credentials {
    username = "username"
    username = ${?BAILE_USERNAME}

    password = "password"
    password = ${?BAILE_PASSWORD}
  }

  request-retry-count = 3
}

argo {
  host = "argo.internal.dev.deepcortex.ai"
  host = ${?ARGO_HOST}

  port = "80"
  port = ${?ARGO_PORT}

  api-version = "v1"
  api-version = ${?ARGO_API_VERSION}

  credentials {
    username = "auth-user"
    username = ${?ARGO_USERNAME}

    password = "auth-password"
    password = ${?ARGO_PASSWORD}
  }

  response-timeout = 5 seconds
}
