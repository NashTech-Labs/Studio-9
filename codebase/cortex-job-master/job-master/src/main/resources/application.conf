cortex-service {
  name = "cortex-job-master"
  heartbeat-interval = 30
  heartbeat-interval = ${?HEARTBEAT_INTERVAL}
}

# See repository deepcortex/cortex-job-master-tasks
cortex-job-master-tasks {
  version = "a54f5f2" //commit hash
  version = ${?CORTEX_TASK_VERSION}

  registry = ""
  registry = ${?CORTEX_TASK_REGISTRY_PREFIX}
}

baile-url = "http://baile/v2.0"
baile-url = ${?BAILE_URL}

sql-server-url = "http://sql-server/v1.0"
sql-server-url = ${?SQL_SERVER_URL}

# Options: OFF, ERROR, WARNING, INFO, DEBUG
akka-loglevel = "DEBUG"
akka-loglevel = ${?AKKA_LOGLEVEL}

akka {
  loglevel = ${akka-loglevel}
  log-dead-letters = 10
  log-dead-letters-during-shutdown = on
  jvm-exit-on-fatal-error=off

  loggers = ["akka.event.slf4j.Slf4jLogger"]
  logging-filter = "akka.event.slf4j.Slf4jLoggingFilter"

  actor {
    debug {
      receive = on
      autoreceive = on
      lifecycle = off
      event-stream = off
    }
    info {
      receive = off
      autoreceive = off
      lifecycle = off
      event-stream = off
    }
  }
}

op-rabbit {
  topic-exchange-name = "ml_job.gateway"
  channel-dispatcher = "op-rabbit.default-channel-dispatcher"
  default-channel-dispatcher {
    # Dispatcher is the name of the event-based dispatcher
    type = Dispatcher

    # What kind of ExecutionService to use
    executor = "fork-join-executor"

    # Configuration for the fork join pool
    fork-join-executor {
      # Min number of threads to cap factor-based parallelism number to
      parallelism-min = 2

      # Parallelism (threads) ... ceil(available processors * factor)
      parallelism-factor = 2.0

      # Max number of threads to cap factor-based parallelism number to
      parallelism-max = 4
    }
    # Throughput defines the maximum number of messages to be
    # processed per actor before the thread jumps to the next actor.
    # Set to 1 for as fair as possible.
    throughput = 10
  }
  connection {
    virtual-host = "/"
    hosts = ["127.0.0.1"]
    username = "guest"
    password = "guest"
    port = 5672
    ssl = false
    connection-timeout = 3s
  }
}

s3-params {
  access-key = ""
  access-key = ${?S3_ACCESS_KEY}
  secret-key = ""
  secret-key = ${?S3_SECRET_KEY}
  region = ""
  region = ${?S3_REGION}
  bucket = ""
  bucket = ${?S3_BUCKET}
}

dremio-params {
  url = "http://dremio"
  url = ${?DREMIO_URL}
  username = "deepcortex"
  username = ${?DREMIO_USERNAME}
  password = ""
  password = ${?DREMIO_PASSWORD}
  source = "s3-source"
  source = ${?DREMIO_SOURCE}
  namespace = "namespace"
  namespace = ${?DREMIO_NAMESPACE}
  s3-tables-dir = "tables"
  s3-tables-dir = ${?S3_TABLES_DIR}
}

redshift-params {
  host = "redshift.amazonaws.com"
  host = ${?REDSHIFT_HOST}
  port = "5439"
  port = ${?REDSHIFT_PORT}
  database = "test"
  database =  ${?REDSHIFT_DATABASE}
  username = "deepcortex"
  username = ${?REDSHIFT_USERNAME}
  password = ""
  password = ${?REDSHIFT_PASSWORD}
  s3-iam-role = ""
  s3-iam-role = ${?REDSHIFT_S3_IAM_ROLE}
}

image-uploading-job {
  block-size = 250,
  additional-task-size = 8192.0,
  max-task-mem-size = 512.0,
  parallelization-factor = 10,
  min-group-size = 10,
  image-max-size = 30.0,
  cpus = 2
}

video-uploading-job {
  cpus = 1,
  task-memory-limit = 8192.0,
  block-size = 250
}

online-prediction-job {
  max-predictions-per-result-file = 10000
}

autoencoder-job {
  cpus = 3.5,
  task-memory-limit = 51200.0,
  gpus = 1
}

classification-job {
  cpus = 3.5,
  task-memory-limit = 51200.0,
  gpus = 1
}

localization-job {
  cpus = 3.5,
  task-memory-limit = 51200.0,
  gpus = 1,
  feature-extractor-task-gpus = 1,
  compose-video-task-memory-limit = 8192.0
}

custom-model-job {
  cpus = 3.5,
  task-memory-limit = 51200.0,
  gpus = 1
}

column-statistics-job {
  cpus = 2.0,
  task-memory-limit = 8000.0
}

analyse-csv-job {
  cpus = 2.0,
  task-memory-limit = 8000.0
}

copier-job {
  cpus = 1.0,
  task-memory-limit = 512.0
}

cross-validation-job {
  cpus = 1.0,
  task-memory-limit = 5120.0
}

redshift-exporter-job {
  cpus = 2.0,
  task-memory-limit = 3072.0
}

splitter-job {
  cpus = 2.0,
  task-memory-limit = 2048.0
}

tabular-job {
  cpus = 2.0,
  task-memory-limit = 16384.0,
  k-folds = 3,
  num-hyper-param-samples = 20
}

redshift-importer-job {
  cpus = 2.0,
  task-memory-limit = 3072.0
}

data-augmentation-job {
  cpus = 1,
  task-memory-limit = 8192.0
}

model-import-job {
  cpus = 2.0,
  task-memory-limit = 8192.0
}

project-packager-job {
  cpus = 1.0,
  task-memory-limit = 8192.0
}

tabular-model-import-job {
  cpus = 2.0,
  task-memory-limit = 3072.0
}

pipeline-runner-job {
  cpus = 2,
  task-memory-limit = 8192.0,
  gpus = 1
}

dremio-importer-job {
  cpus = 2.0,
  task-memory-limit = 3072.0
}

dremio-exporter-job {
  cpus = 2.0,
  task-memory-limit = 3072.0
  chunksize = 1000000
}

dataset-transfer-job {
  parallelization-factor = 10,
  min-group-size = 10,
  image-max-size = 4096.0,
  cpus = 1.0,
  memory = 512.0
}
