pegasus-service {
  name = "pegasus-api"
  version = "v1"
}

akka {
  # Options: OFF, ERROR, WARNING, INFO, DEBUG
  loglevel = "DEBUG"
  loglevel = ${?AKKA_LOGLEVEL}

  log-dead-letters = 10
  log-dead-letters-during-shutdown = on
  jvm-exit-on-fatal-error=off

  loggers = ["akka.event.slf4j.Slf4jLogger"]
  logging-filter = "akka.event.slf4j.Slf4jLoggingFilter"

  actor {
    debug {
      receive = on
      autoreceive = on
      lifecycle = on
      event-stream = off
    }
    info {
      receive = off
      autoreceive = off
      lifecycle = off
      event-stream = off
    }
  }

  extensions = ["pegasus.service.data.S3Settings"]
}

akka.http {
  server {
    server-header = ${pegasus-service.name}/${pegasus-service.version}
    idle-timeout = 60 s
    request-timeout = 120 s
    max-connections = 1024
  }

  client {
    connecting-timeout = 120 s
  }

  host-connection-pool {
    max-connections = 1024
    max-retries = 3
    max-open-requests = 16384
  }
}

http {
  interface = "0.0.0.0"
  port = 9000

  auth-user-name = "auth-user"
  auth-user-name = ${?HTTP_AUTH_USER_NAME}

  auth-user-password = "auth-password"
  auth-user-password = ${?HTTP_AUTH_USER_PASSWORD}
}

aws {
  s3 {
    region = "us-west-2"
    region = ${?AWS_S3_REGION}

    credentials {
      access-key-id = "access-key-id"
      access-key-id = ${?AWS_S3_ACCCESS_KEY_ID}

      secret-access-key = "secret-access-key"
      secret-access-key = ${?AWS_S3_SECRET_ACCESS_KEY}
    }
  }

  redshift {
    url = "jdbc:redshift://examplecluster.canhfkm0rqdk.us-west-2.redshift.amazonaws.com:5439/dev"
    url = ${?AWS_REDSHIFT_URL}

    user = "masteruser"
    user = ${?AWS_REDSHIFT_USER}

    password = "Masterpassword1"
    password = ${?AWS_REDSHIFT_PASSWORD}
  }
}

slick {
  redshift {
    url = ${aws.redshift.url}
    user = ${aws.redshift.user}
    password = ${aws.redshift.password}
    driver = com.amazon.redshift.jdbc.Driver
    connectionPool = "HikariCP"
    poolName = "PegagusHikariPool"
    connectionInitSql = "SELECT TRUE"
    maximumPoolSize = 20
    maxLifetime = 3600000 //60 min session lifetime
  }
}

op-rabbit {
  topic-exchange-name = "ml_job.gateway"
  channel-dispatcher = "op-rabbit.default-channel-dispatcher"
  default-channel-dispatcher {
    # Dispatcher is the name of the event-based dispatcher
    type = Dispatcher

    # What kind of ExecutionService to use
    executor = "fork-join-executor"

    # Configuration for the fork join pool
    fork-join-executor {
      # Min number of threads to cap factor-based parallelism number to
      parallelism-min = 2

      # Parallelism (threads) ... ceil(available processors * factor)
      parallelism-factor = 2.0

      # Max number of threads to cap factor-based parallelism number to
      parallelism-max = 4
    }
    # Throughput defines the maximum number of messages to be
    # processed per actor before the thread jumps to the next actor.
    # Set to 1 for as fair as possible.
    throughput = 10
  }
  connection {
    virtual-host = "/"

    hosts = ["127.0.0.1"]
    hosts = [${?RABBIT_HOSTS}]

    username = "guest"
    username = ${?RABBIT_USERNAME}

    password = "guest"
    password = ${?RABBIT_PASSWORD}

    port = 5672
    ssl = false
    connection-timeout = 3s
  }
}
